<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Efficient Video Inference with Spatiotemporal Knowledge -- Chanwut (Mick) Kittivorawong</title>

    <link rel="stylesheet" href="dist/reset.css">
    <link rel="stylesheet" href="dist/reveal.css">
    <link rel="stylesheet" href="dist/theme/blackonwhite.css">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css"
        type="text/css" ?>
    <script src="https://code.jquery.com/jquery-3.7.1.slim.min.js"
        integrity="sha256-kmHvs0B+OpCW5GVHUNjv9rOmY0IvSIRcf7zGUDTDQM8=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/@unocss/runtime"></script>
    <link rel="stylesheet" type="text/css" href="styles.css">
    <link rel="stylesheet" href="plugin/diagram/revealjs-diagram.css">

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/stackoverflow-dark.min.css">
</head>

<section data-background-color="black"></section>
<section vcenter>
    <h2 class="text-left">Efficient Video Inference with Spatiotemporal Knowledge</h2>
    <h4 class="text-color">Chanwut (Mick) Kittivorawong</h4>
    <div class="flex justify-between w-2/5 h-35pt">
        <img src="assets/logo-berkeley.svg" class="h-full !m-0" />
        <img src="assets/logo-sky.png" class="h-full !m-0 !mr-2" />
        <img src="assets/logo-dsf.svg" class="h-full !m-0" />
    </div>
    <aside class="notes" data-markdown>
        - Hi everyone
        - My name is Mick. I am a 4th-year PhD student advised by Professor Alvin Cheung.
        - Today, I am excited to tell you about my research on Efficient Video Inference with Spatiotemporal Knowledge.
    </aside>
</section>
<section>
    <h2>Overview</h2>
    <br />
    <br />
    <columns>
        <div class="w-[55%]">
            <div class="fragment custom fade">
                <h3 class="line-height-1">Motivation</h3>
                <div class="text-color text-3xl">* Challenges with video inferencing.
                    <br />
                    &nbsp;
                </div>
            </div>
            <br />
            <div class="fragment custom fade">
                <h3>Background + Prior Work</h3>
                <div class="text-color text-3xl">* Basic video processing tasks.</div>
                <div class="text-color text-3xl">* Prior work done to accelerate video processing.</div>
            </div>
        </div>
        <div>
            <div class="fragment custom fade">
                <h3>Spatialyze</h3>
                <div class="text-color text-3xl">* A Geospatial Video Analytics System.</div>
                <div class="text-color text-3xl">* Programming Model &amp; Optimization.</div>
            </div>
            <br />
            <div class="fragment custom fade">
                <h3>Ongoing&nbsp;Research</h3>
                <div class="text-color text-3xl">* Toward more general solutions.</div>
            </div>
        </div>
    </columns>
    <aside class="notes" data-markdown>
        - As an overview of this talk, I will
        - First discuss, the motivation behind my research and the challenges with video inferencing.
        - Then, I will touch on the background and prior work and give you a general idea of the scope of my research.
        - Next, I will talk about Spatialyze, a video analytics system we built and published at VLDB.
        - And finally, I will discuss the restriction of spatialyze and the ongoing research to make it more general.
    </aside>
</section>
<section>
    <h3>Motivation</h3>
    <h4>Video data is abundant, and its volume keeps increasing.</h4>
    <br />
    <div class="flex justify-center w-[100%]">
        <img src="assets/youtube-upload.svg" style="padding: 0; margin: 0" />
    </div>
    <div class="absolute left-0 bottom-0 text-[8.5pt] color-gray">Hours of video uploaded to YouTube every
        minute as of February 2022 [Graph], YouTube, & Google, June 22, 2022. [Online]. Available:
        https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute</div>
    <aside class="notes" data-markdown>
        - The first motivation of my research is the abundance of video data.
        - Based on the statistics from statista, in 2019, the amount of videos uploaded to youtube reached 500 hours per
        minute.
        - Therefore, an efficient tool for analyzing these video data is necessary.
    </aside>
</section>
<section>
    <h3>Motivation</h3>
    <columns>
        <column>
            <p>Variety of video data sources.</p>
            <img style="background-color: black" class="b-rd-4px p-12px w-500px !mb-4" src="assets/data-nuscenes.svg" />
            <img style="background-color: black" class="b-rd-4px p-12px w-500px !mb-4" src="assets/data-i24.png" />
            <img style="background-color: black" class="b-rd-4px p-12px w-500px !mb-4" src="assets/data-bdd.png" />
        </column>
        <column>
            <p class="fragment">With real-world use cases.</p>
            <div>
                <div class="fragment">
                    <img src="assets/data-ex-nuscenes.gif" class="b-rd-4px w-500px !pb-0 !mb-0" />
                    <p class="text-xl p-0 m-0">Finding cars at an intersection.</p>
                </div>
                <div class="fragment">
                    <img src="assets/data-ex-jackson.gif" class="b-rd-4px w-500px !pb-0 !mb-0" />
                    <p class="text-xl p-0 m-0">Aggregate vehicle statistics on traffic intersections.</p>
                </div>
                <div class="fragment">
                    <img src="assets/data-ex-b3d.gif" class="b-rd-4px w-500px !pb-0 !mb-0" />
                    <p class="text-xl p-0 m-0">Building a statistical model on vehicles coordination.</p>
                </div>
            </div>
        </column>
    </columns>
    <aside class="notes" data-markdown>
        - In addition, to videos from youtube, there are also a variety of video data sources.
        - For exmaple, these datasets contain videos ranging from self-driving cars, traffic cameras, and drones'
        cameras.
        - These videos are used in real-world use cases.
        - where an autonomous vehicle researcher may want to find a crash scene at an intersection from a terrabytes of
        AV footages.
        - or, a civil engineer may want to aggregate vehicle statistics on traffic intersections.
        - or, a traffic engineer may want to build statistical models on vehicles coordination.
        - However, working with these videos is challenging.
    </aside>
</section>
<section data-auto-animate>
    <h3>Challenge #1</h3>
    <h4>The overwhelming diversity of interfaces for tools for video analytics.</h4>
    <br />
    <columns>
        <column>
            <div data-markdown>
                ### Video En-/De-coder
                - FFmpeg
                - OpenCV
            </div>
            <br />
            <br />
            <div data-markdown class="fragment">
                ### Object Detector
                - Mask-RCNN
                - RetinaNet
                - YOLOv5
            </div>
        </column>
        <column>
            <div data-markdown class="fragment">
                ### Object Tracker
                - SORT
                - DeepSORT
                - StrongSORT
            </div>
            <br />
            <div data-markdown class="fragment">
                ### Query Processor
                - MobilityDB
                - PostGIS
            </div>
        </column>
    </columns>
    <aside class="notes" data-markdown>
        - The first challenge of video analytics is the overwhelming amount of tools and their interfaces for video
        analytics.
        - It is unclear how to integrate these tools together to form a video analytics pipeline.
        - For example, ffmpeg decode a video file into a stream of bytes, while OpenCV can decode a video file into a
        sequence of numpy arrays.
        - All object detectors have different output format.
        - DeepSORT and StrongSORT requires video frame input, while SORT does not.
        - Finally, MobilityDB provides helpful utility functions for trajectories data, while PostGIS provides a more
        standardized interface for GIS.
        - Each tools are developed independently and is not necessarily developed for video analytics in mind.
        - So, this leads to a lot of overhead in integrating these tools together when constructing a video analytics
        pipeline.
    </aside>
</section>
<section>
    <h3>Challenge #2</h3>
    <div>ML models to process video data are still slow.</div>
    <br /> <br />
    <columns>
        <column data-markdown>
            | Object Detector | FPS |
            | --- | ---: |
            | Mask-RCNN | 4.3 |
            | RetinaNet (R101+FPN) | 14.3 |
            | YOLOv5 | 90.1 |
        </column>
        <column data-markdown class="fragment">
            | Object Tracker | FPS |
            | --- | ---: |
            | StrongSORT | 1.5 |
            | DeepSORT | 3.2 |
        </column>
    </columns>
    <div class="absolute left-0 bottom-0 text-[8pt] color-gray">Yunhao Du, Zhicheng Zhao, Yang Song, Yanyun
        Zhao, Fei Su, Tao Gong, and Hongying Meng. 2023. StrongSORT: Make DeepSORT Great Again. IEEE
        Transactions on Multimedia 25, (2023), 8725-8737. DOI:https://doi.org/10.1109/TMM.2023.3240881
        <br />
        Jaeho Bang, Gaurav Tarlok Kakkar, Pramod Chunduri, Subrata Mitra, and Joy Arulraj. 2023. Seiden:
        Revisiting Query Processing in Video Database Systems. Proc. VLDB Endow. 16, 9 (May 2023),
        2289-2301. https://doi.org/10.14778/3598581.3598599
    </div>
    <aside class="notes" data-markdown>
        - The second challenge is the speed of ML models to process video data.
        - With the amount of video data available, current ML models to process them are still too slow.
        - For example, Mask-RCNN can only process 4.3 frames per second.
        - RetinaNet can process 14.3 frames per second.
        - YOLOv5 can process 90.1 frames per second, but is struggle with accuracy for small objects.
        - Even worse, StrongSORT can only process 1.5 frames per second.
        - while DeepSORT can process 3.2 frames per second.
    </aside>
</section>
<section vcenter>
    <p>Main Research Question</p>
    <h2>How can we build an end-to-end system for efficiently processing video queries?
        <br />
        &nbsp;
    </h2>
    <aside class="notes" data-markdown>
        - With these challenges in mind:
        - How can we build an end-to-end system for efficiently processing video queries?
    </aside>
</section>
<section>
    <h3>Background</h3>
    <h4>Video analytics tasks and their bottleneck (Detection Query).</h4>
    <br />
    <div class="r-stack">
        <img src="assets/od-pipeline-1.svg" class="w-100%" />
        <img src="assets/od-pipeline-2.svg" class="w-100% fragment" />
    </div>
    <aside class="notes" data-markdown>
        - To paint a better picture of what type of video analytics tasks we are dealing with.
        - Here is a simplified pipeline for object detection query.
        - The pipeline starts with a video file decoded as a sequence of image frames input into an object detector.
        - The object detector then outputs a list of bounding boxes.
        - The bounding boxes are then input into a query processor to answer the users' query.
    </aside>
</section>
<section>
    <h3>Background</h3>
    <h4>Video analytics tasks and their bottleneck (Tracking Query).</h4>
    <br />
    <div class="r-stack">
        <img src="assets/ot-pipeline-1.svg" class="w-100%" />
        <img src="assets/ot-pipeline-2.svg" class="w-100% fragment" />
        <img src="assets/ot-pipeline-3.svg" class="w-100% fragment" />
    </div>
    <aside class="notes" data-markdown>
        - Similarly, here is a implified pipeline for object tracking query.
        - The pipeline starts with a video file decoded as a sequence of image frames input into an object tracker.
        - The object tracker then outputs a list of object tracks.
        - The tracks are then input into a query processor to answer different types of queries related to the tracks.
        - Since the bottlenecks of these pipelines are the object detector and the object tracker, we will focus on
        optimizing these two components.
    </aside>
</section>
<section>
    <h3>Prior Work</h3>
    Most of existing optimization techniques are query-specific.
    <br />
    <br />
    <div data-markdown>
        <textarea data-template>
        ```sql [1-7|3|7]
        --- Retrieval Query
        SELECT frame_id FROM UA-DETRAC
        WHERE COUNT(TRUCK) > 0 WITH PRECISION > 0.95;

        --- Aggregate Query
        SELECT AVG(COUNT(CAR)) FROM UA-DETRAC
        WITH CONFIDENCE > 95% AND ERROR < 0.2;
        ```
        </textarea>
    </div>
    <br />
    <columns>
        <column data-markdown>
            **Retrieval Query**

            NoScope (Kang, 2017)
        </column>
        <column data-markdown>
            **Aggregate Query**

            BlazeIt (Kang, 2019)
        </column>
        <column data-markdown>
            **Both**

            TASTI (Kang, 2022)

            Seiden (Bang, 2023)
        </column>
    </columns>
    <aside class="notes" data-markdown>
        - Most of the existing optimization techniques are query-specific.
        - Where the final results of the pipeline are to retrieve frames or compute statistics and restricted by some
        error constraints.
        - NoScope, BlazeIt, and TASTI make use of small proxy models to act as both video and query processor.
        - Seiden instead uses the temporal properties of videos to propagate query results between nearby video frames.
        - However, we find that these types of optimization techniques are quite restrictive to only specific types of
        queries.
    </aside>
</section>
<section vcenter>
    <div data-markdown>
        ## Spatialyze
        - An end-to-end geospatial video analytics system.
        - Leverage spatiotemporal properties to sample only relevant frames.
    </div>

    <br />

    <div data-markdown class="fragment custom fade">
        ## Ongoing Research
        - Focus on video processing.
        - Combine proxy models with spatiotemporal knowledge.
    </div>
    <aside class="notes" data-markdown>
        - Therefore, I would like to present my approach for building an end-to-end system for efficiently video
        analytics.
        - In the first project in this research, we built a geospatial video analytics system
        - Where users analyze videos together with their geospatial data.
        - And our system optimize the video processing pipeline based on the users' queries
        - In the ongoing research, we shifted our focus to video processing pipeline.
        - We are combining proxy models with spatiotemporal knowledge to accelerate video inference.
    </aside>
</section>
<section vcenter data-markdown>
    # Spatialyze
</section>
<section>
    <h3>Issues with Spatialyze.</h3>
    <br />
    <br />
    <columns>
        <column data-markdown class="fragment fade-in-then-semi-out">
            <textarea data-template>
            ```pt




            w = World()
            rn = RoadNetwork('./road')
            w.addGeogConstructs(rn)
            ```
            #### Requires user-provided geospatial metadata.
            </textarea>
        </column>
        <column data-markdown class="fragment fade-in-then-semi-out">
            <textarea data-template>
            ```pt

            w.addVideo(GeospatialVideo(
                Video('v0.mp4'),
                Camera('c0.json')))
            w.addVideo(GeospatialVideo(
                Video('v1.mp4'),
                Camera('c1.json')))
            ```
            #### Restricted to only geospatial video analytics.
            </textarea>
        </column>
        <column data-markdown class="fragment fade-in">
            <textarea data-template>
            ```pt
            o = w.object()
            c = w.camera()
            inter = w.geogConstruct(
                type='intersection')
            w.filter((o.type == 'car')
              & (distance(o, c) < 50)
              & contains(inter, o))
            ```
            #### Relies on users' queries to optimize its video processing pipeline.
            </textarea>
        </column>
    </columns>
</section>
<section vcenter>
    <columns>
        <column>
            <h4>Video inference is <mark class="fragment custom hl-red" data-fragment-index="1">slow</mark>.</h4>
        </column>
        <column>
            <h4>Prior work proposes acceleration solutions with significant trade-off of being <mark
                    class="fragment custom hl-yellow" data-fragment-index="2">query-restricted</mark> or <mark
                    class="fragment custom hl-yellow" data-fragment-index="2">domain-restricted</mark>.</h4>
        </column>
    </columns>
    <br />
    <h3>We integrate both <mark class="fragment custom hl-green" data-fragment-index="3">spatiotemporal knowledge</mark>
        and <mark class="fragment custom hl-green" data-fragment-index="3">proxy model</mark> approach together to allow
        both domain- and query-flexibility in video inference acceleration.
    </h3>
</section>
<section>
    <h3>Prior Work</h3>
    <h4>Proxy-based approach.</h4>
</section>

<body>
    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>

    <script src="https://d3js.org/d3.v3.min.js"></script>
    <script src="plugin/diagram/revealjs-diagram.js"></script>

    <script>
        (function () {
            const SCALE = 85;
            $('body').prepend($('<div class="reveal">').append($('<div class="slides">')))
            $('.reveal .slides').append(...$('section'))

            $('columns').each((_, columns) => {
                const nColumns = $(columns).children().length
                const nSpaces = nColumns - 1
                const width = (100 - (nSpaces * 2)) / nColumns
                const style = `width: ${width}%;`
                $(columns)
                    .children()
                    .filter('column')
                    .each((_, column) => column.setAttribute('style', style))
            })
            Reveal.initialize({
                hash: true,

                transition: 'none',
                backgroundTransition: 'none',
                slideNumber: 'c',
                controls: false,
                progress: false,

                width: 16 * SCALE,
                height: 9 * SCALE,
                markdown: {
                    gfm: true,
                },

                plugins: [
                    RevealMarkdown,
                    RevealHighlight,
                    RevealNotes,
                ],
            })
        })()
    </script>
</body>

</html>